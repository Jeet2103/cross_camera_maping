# Brief Report: Cross-View Player Mapping & Re-identification System

The objective of this project is to build an advanced cross-view player tracking and re-identification system that effectively maps and tracks players across broadcast and tactical (top-down) camera views. In sports analytics, especially in football or basketball, player identities often become ambiguous when switching between different camera feeds. This system addresses that challenge by ensuring consistent player identification across disjoint views, enabling deeper insights into player movements, formations, and tactical decisions.

## Approach and Methodology

### 1. Player Detection with YOLOv11

- **Objective:**
For precise and high-speed detection of players, the system utilizes the YOLOv11 (You Only Look Once version 11) object detection framework—renowned for its cutting-edge performance on both GPU and edge devices.

- **Model Used:** A fine-tuned YOLOv11 model named best.pt, trained on a sports-specific dataset to recognize multiple classes relevant to the game:
    - Player, Goalkeeper, Referee, and Ball.
- **Methodology:**

    - **Multi-Class Detection:** Accurately detects and differentiates between players, goalkeepers, referees, and the ball in real-time.

    - **Color-Coded Bounding Boxes:** Each class is rendered with a distinct bounding box color for intuitive visual feedback.

    - **Advanced Detection Head:** Enhanced localization and classification, especially in high-density scenes with overlapping individuals.

    - **Temporal Consistency:** The detector has been optimized to maintain consistent detection across frames, reducing ID switches and missed detections in fast-paced scenes.

- **Output:**
    - Detected bounding boxes with class labels (`player`, `goalkeeper`, `referee`, `ball`) and confidence scores.

    - For both the broadcast view and the tactical view videos, the detection module automatically generates two CSV files with the following format: `frame_id`, `x1`, `y1`, `x2`, `y2`, `confidence_score`, `class`

    - These outputs are forwarded to the tracking pipeline for identity assignment and continuity across frames.

- **Implementation File:**
    - **Filename:** `detect_players.py`

### 2. Player Tracking with StrongSORT

- **Objective:**
To ensure consistent identification of players across time, the system implements StrongSORT — a powerful real-time multi-object tracking (MOT) framework. It is a robust extension of DeepSORT, designed specifically to handle the challenges of dense scenes, occlusions, and rapid movements commonly observed in sports footage.

- **Methodology:**
    - **Input:** CSVs from `detect_players.py` containing detections (`frame ID`, `bounding boxes`, `confidence`, `class`).

    - **Motion Modeling:** Uses a Kalman Filter to estimate and predict player positions frame by frame.

    - **Appearance Embedding:** Integrates ReID features to preserve identity even during occlusions or overlaps.

    - **Track ID Assignment:** Assigns a unique track_id to each player per view that remains consistent as long as the player is visible.

- **Output:**
    - Generates two tracking CSVs, one for each video containing `frame_id`, `track_id`, `x1`, `y1`, `x2`, `y2`, `class`
    - Used for further cross-view matching and features extraction.
    - Each tracking step is logged using the custom `logger.py` for transparency and debugging.
- **Implementation File:**
    - **Filename:** `track_players.py`

### 3. Appearance & Spatial Feature Extraction with OSNet

- **Objective:**
To accurately match players across different camera views, the system extracts appearance and spatial features using a powerful feature extraction pipeline based on the OSNet model and homography transformation.
- **Methodology:**
    - **Appearance Feature Extraction:** The system uses the `OSNet_x1_0` model from Torchreid's FeatureExtractor to generate high-dimensional appearance embeddings for each player by cropping their regions from sampled video frames.

    - **Tracking Data Processing:** Player tracking data, including bounding box coordinates and IDs, is read from CSV files generated by `track_players.py`.

    - **Spatial Alignment via Homography:** Each bounding box’s center point is transformed using a precomputed homography matrix to align spatial features across multiple camera views.

    - **Feature Aggregation & Logging:** Appearance and spatial features are averaged across frames to form a compact player representation, with all operations and exceptions logged using a custom logger (`extract_features.log``).

- **Output:**
    - This module generates two `.npy` files (`broadcast_features.npy` and `tacticam_features.npy`) stored in the `mapping/features/ directory`. Each entry maps a unique `track_id` to a feature vector that includes both appearance and spatial information. These files are critical for downstream cross-view player matching.
- **Implementation File**
    - **File Name**: `extract_features.py`

### 4. Cross-View Player ID Matching

- **Objective:**
This module aims to establish a one-to-one mapping between players detected and tracked in two distinct camera views—broadcast and tacticam. The goal is to maintain consistent player identity across views for enhanced analytics and tactical insights.
- **Methodology:**
The system loads player-level appearance and spatial features from two .npy files generated by `extract_features.py`. Each player's feature vector is split into two parts: a 2048-dimensional appearance embedding and a 2D spatial coordinate (`transformed via homography`).
To match players:

    - Features are normalized and concatenated.

    - A cost matrix is computed using a weighted combination of appearance similarity (`cosine distance`) and spatial proximity (`Euclidean distance`).

    - The Hungarian algorithm (`linear_sum_assignment`) is used to optimally solve the assignment problem.

Thresholding is applied to eliminate weak matches. Each accepted mapping is logged with a confidence score, and unmatched players are skipped with a warning for transparency.

- **Output:**
    - A JSON file `id_mapping.json` is saved in the `mapping/` directory, mapping each `track_id` from the tacticam video to its corresponding ID in the broadcast view. This mapping ensures identity consistency for visualization and further tactical interpretation.

    - All processing steps, score thresholds, and exceptions are traced via `match_players.log`.

- **Implementation File:**
    - **Filename:** `match_players.py`

### 5. Visualizing Final Player Tracking and Cross-View Mapping
- **Objective:**
This module visually presents the player tracking results for both camera views (broadcast and tacticam), integrating cross-view player ID mappings. The aim is to create clear, annotated videos that reflect consistent player identities across perspectives.

- **Methodology:**
The system loads tracking data from CSV files (`broadcast_tracks.csv` and `tacticam_tracks.csv`) and a previously generated player ID mapping (`id_mapping.json`).
For each frame in both videos:

    - Bounding boxes are drawn around each detected player.

    - Player IDs are overlaid:

        - For broadcast, original track IDs are used.

        - For tacticam, mapped IDs from the broadcast view are shown in green if available.

        - Unmatched tacticam players are labeled NoMap:{`track_id`} in red.

All processing is logged via `visualize_results.log`. The frame-wise annotations help verify accuracy of tracking and cross-view alignment.

- **Output:**
Two annotated videos are generated and saved in the `output/` directory:

    - `broadcast_output.mp4`

    - `tacticam_output.mp4`
These outputs are essential for visual evaluation of multi-camera player identity consistency.

- **Implementation File:**
    - **Filename:** `visualize_results.py`

## Techniques Explored and Their Outcomes

### Successful Approaches

- **YOLOv5 (Torchvision-based) for Player Detection:**
Utilized pretrained weights for robust player detection in both broadcast and tacticam videos. Achieved high precision in complex and crowded scenes.

- **StrongSORT for Object Tracking:**
Outperformed DeepSORT in maintaining consistent track_ids over time. Better handled occlusions, re-identification, and rapid player movements in sports scenarios.

- **OSNet for Re-Identification (ReID) Feature Extraction:**
Integrated with torchreid, OSNet provided compact and discriminative appearance embeddings crucial for player identity preservation.

- **Homography Transformation for Spatial Alignment:**
Enabled mapping of player coordinates across camera views, crucial for cross-view consistency.

- **FAISS + Hungarian Algorithm for Cross-View Player Mapping:**
Combined cosine similarity of appearance embeddings with spatial distance for accurate player matching. Weighted combination (0.7 appearance, 0.3 spatial) significantly improved results.

- **OpenCV for Visualization and Result Validation:**
Generated annotated video outputs with bounding boxes and cross-mapped IDs to verify tracking and identity alignment.

- **TensorBoard for Feature Embedding and Performance Monitoring:**
Used to visualize training dynamics and embeddings to assess model convergence and separation of identities.

- **Scikit-learn for Metric Evaluation and Similarity Computation:**
Applied for L2 normalization, cosine similarity validation, and clustering metrics during evaluation phases.

### Techniques Explored But Not Adopted

- **DeepSORT for Tracking:**
Initially integrated for temporal tracking but failed to maintain stable identities under frequent occlusions and dense group interactions, leading to frequent ID switches.

- **EfficientNet-B3 for Feature Extraction:**
Tried as an alternative to OSNet for ReID, but failed to generate sufficiently discriminative features for cross-player matching. Resulted in low identity recall.

- **Pure Cosine Similarity for Cross-View Mapping:**
Attempted direct cosine similarity matching between broadcast and tacticam features. Performed poorly due to the lack of spatial context and camera viewpoint disparity.

### Tech Stack Overview

- **Frameworks & Libraries:** PyTorch, Torchvision, OpenCV, Scikit-learn, TensorBoard, Torchreid, FAISS, SciPy

- **Models & Algorithms:** YOLOv5, StrongSORT, OSNet, EfficientNet-B3, DeepSORT, Hungarian Algorithm, Homography

- **Programming Language:** Python

- **Logging & Monitoring:** Custom logger using logging, TensorBoard


## Challenges Encountered

    1. **Tracking Instability in Crowded Sports Scenarios:**
While using DeepSORT as an initial tracking baseline, the model frequently lost track of player identities during occlusions and rapid movements. In fast-paced environments like football matches, ID switches and track fragmentation were common—reducing tracking reliability over time.

        2. **Poor Generalization of EfficientNet-B3 for Appearance Re-ID:**
Despite its strong performance in generic classification tasks, EfficientNet-B3 failed to generate discriminative enough embeddings for player re-identification across two camera angles. The extracted features lacked consistency under varying poses and resolutions, leading to low cross-view matching accuracy.

    3. **Ineffectiveness of Cosine Similarity Alone for Cross-View Matching:**
An early attempt to match players across views using only cosine similarity of appearance features resulted in poor precision. Appearance features alone couldn't account for changes in viewpoint, scale, and lighting—especially with broadcast and tacticam having drastically different perspectives.

    4. **Homography Limitations in Non-Planar Fields:**
Although homography transformation aligned player coordinates reasonably well, slight misalignments due to field curvature, camera lens distortion, and height differences in player position caused errors in spatial feature calculation—affecting downstream FAISS-based matching.

    5. **Real-Time Visualization Bottlenecks:**
Rendering annotated videos with multiple players in high resolution posed performance bottlenecks. Using OpenCV’s rendering pipeline in CPU mode was computationally expensive, slowing down visual debugging and making batch visualization less efficient.

    6. **Noise in Bounding Box Crops During Feature Extraction:**
Small bounding boxes or poor-quality crops (e.g., partial players or overlapping objects) affected the reliability of ReID feature vectors. This introduced noise into the embedding space and skewed player similarity scores.

    7. **Data Synchronization Between Camera Views:**
Broadcast and tacticam videos often had unsynchronized frame indices or dropped frames. Aligning them for consistent frame-by-frame comparison was a manual and error-prone task that occasionally introduced inconsistencies in tracking and mapping.

## Future Enhancements & System Improvements